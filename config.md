# Spark Configuration

The exact explanation and defaults for spark config can be found [here](https://spark.apache.org/docs/latest/configuration.html), `None` means to use the spark native defaults

# Config PySpark Session via environment variables

> Generated by [generate-config-docs.py](./generate_config_docs.py)
> Run `python ./generate_config_docs.py` to update this file

Source code: SPARK_CONNECT/config/configer.py

Avaliable environment variables for SparkEnvConfiger:

Default config:

- `SPARK_CONNECT_APP_NAME`: `spark.app.name`, default: `SPARK_CONNECT`.
- `SPARK_CONNECT_DEPLOY_MODE`: `spark.submit.deployMode`, default: `client`.
- `SPARK_CONNECT_SCHEDULER_MODE`: `spark.scheduler.mode`, default: `FAIR`.
- `SPARK_CONNECT_UI_PORT`: `spark.ui.port`, default: `None`.
- `SPARK_CONNECT_DRIVER_JAVA_OPTIONS`: `spark.driver.defaultJavaOptions`, default: `None`.
- `SPARK_CONNECT_EXECUTOR_JAVA_OPTIONS`: `spark.executor.defaultJavaOptions`, default: `None`.
- `SPARK_CONNECT_DRIVER_JAVA_EXTRA_OPTIONS`: `spark.driver.extraJavaOptions`, default: `None`.
- `SPARK_CONNECT_EXECUTOR_JAVA_EXTRA_OPTIONS`: `spark.executor.extraJavaOptions`, default: `None`.
- `S3_ACCESS_KEY` or `AWS_ACCESS_KEY_ID`: `spark.hadoop.fs.s3a.access.key`, default: `None`.
- `S3_SECRET_KEY` or `AWS_SECRET_ACCESS_KEY`: `spark.hadoop.fs.s3a.secret.key`, default: `None`.
- `S3_ENTRY_POINT`: `spark.hadoop.fs.s3a.endpoint`, default: `None`.
- `S3_ENTRY_POINT_REGION` or `AWS_DEFAULT_REGION`: `spark.hadoop.fs.s3a.endpoint.region`, default: `None`.
- `S3_PATH_STYLE_ACCESS`: `spark.hadoop.fs.s3a.path.style.access`, default: `None`.
- `S3_MAGIC_COMMITTER`: `spark.hadoop.fs.s3a.bucket.all.committer.magic.enabled`, default: `None`.
- `SPARGIM_KERBEROS_KEYTAB`: `spark.kerberos.keytab`, default: `None`.
- `SPARGIM_KERBEROS_PRINCIPAL`: `spark.kerberos.principal`, default: `None`.

`config_basic()` can config following:

- `SPARK_CONNECT_APP_NAME`: `spark.app.name`, default: `SPARK_CONNECT`.
- `SPARK_CONNECT_DEPLOY_MODE`: `spark.submit.deployMode`, default: `client`.
- `SPARK_CONNECT_SCHEDULER_MODE`: `spark.scheduler.mode`, default: `FAIR`.
- `SPARK_CONNECT_UI_PORT`: `spark.ui.port`, default: `None`.
- `SPARK_CONNECT_DRIVER_JAVA_OPTIONS`: `spark.driver.defaultJavaOptions`, default: `None`.
- `SPARK_CONNECT_EXECUTOR_JAVA_OPTIONS`: `spark.executor.defaultJavaOptions`, default: `None`.
- `SPARK_CONNECT_DRIVER_JAVA_EXTRA_OPTIONS`: `spark.driver.extraJavaOptions`, default: `None`.
- `SPARK_CONNECT_EXECUTOR_JAVA_EXTRA_OPTIONS`: `spark.executor.extraJavaOptions`, default: `None`.

`config_s3()` can config following:

- `S3_ACCESS_KEY` or `AWS_ACCESS_KEY_ID`: `spark.hadoop.fs.s3a.access.key`, default: `None`.
- `S3_SECRET_KEY` or `AWS_SECRET_ACCESS_KEY`: `spark.hadoop.fs.s3a.secret.key`, default: `None`.
- `S3_ENTRY_POINT`: `spark.hadoop.fs.s3a.endpoint`, default: `None`.
- `S3_ENTRY_POINT_REGION` or `AWS_DEFAULT_REGION`: `spark.hadoop.fs.s3a.endpoint.region`, default: `None`.
- `S3_PATH_STYLE_ACCESS`: `spark.hadoop.fs.s3a.path.style.access`, default: `None`.
- `S3_MAGIC_COMMITTER`: `spark.hadoop.fs.s3a.bucket.all.committer.magic.enabled`, default: `None`.

`config_kerberos()` can config following:

- `SPARGIM_KERBEROS_KEYTAB`: `spark.kerberos.keytab`, default: `None`.
- `SPARGIM_KERBEROS_PRINCIPAL`: `spark.kerberos.principal`, default: `None`.

`config_local()` can config following:

- `SPARK_CONNECT_MASTER`: `spark.master`, default: `local[*]`.
- `SPARK_CONNECT_LOCAL_MEMORY`: `spark.driver.memory`, default: `512m`.

`config_connect_client()` can config following:

- `SPARK_CONNECT_REMOTE`: `spark.remote`, default: `sc://localhost:15002`.

`config_connect_server()` can config following:

- `SPARK_CONNECT_CONNECT_SERVER_PORT`: `spark.connect.grpc.binding.port`, default: `None`.
- `SPARK_CONNECT_CONNECT_GRPC_ARROW_MAXBS`: `spark.connect.grpc.arrow.maxBatchSize`, default: `None`.
- `SPARK_CONNECT_CONNECT_GRPC_MAXIM`: `spark.connect.grpc.maxInboundMessageSize`, default: `None`.

`config_k8s()` can config following:

- `SPARK_CONNECT_MASTER`: `spark.master`, default: `k8s://https://kubernetes.default.svc`.
- `SPARK_CONNECT_KUBERNETES_NAMESPACE`: `spark.kubernetes.namespace`, default: `None`.
- `SPARK_CONNECT_KUBERNETES_IMAGE`: `spark.kubernetes.container.image`, default: `wh1isper/spark-executor:3.4.1`.
- `SPARK_CONNECT_KUBERNETES_IMAGE_PULL_SECRETS`: `spark.kubernetes.container.image.pullSecrets`, default: `None`.
- `SPARK_CONNECT_KUBERNETES_IMAGE_PULL_POLICY`: `spark.kubernetes.container.image.pullPolicy`, default: `IfNotPresent`.
- `SPARK_EXECUTOR_NUMS`: `spark.executor.instances`, default: `3`.
- `SPARK_CONNECT_KUBERNETES_EXECUTOR_LABEL_LIST`: `spark.kubernetes.executor.label.*`, default: `SPARK_CONNECT-executor`. A string seperated by `,` will be converted
- `SPARK_CONNECT_KUBERNETES_EXECUTOR_ANNOTATION_LIST`: `spark.kubernetes.executor.annotation.*`, default: `SPARK_CONNECT-executor`. A string seperated by `,` will be converted
- `SPARK_CONNECT_DRIVER_HOST`: `spark.driver.host`, default: `None`.
- `SPARK_CONNECT_DRIVER_BINDADDRESS`: `spark.driver.bindAddress`, default: `0.0.0.0`.
- `SPARK_CONNECT_DRIVER_POD_NAME`: `spark.kubernetes.driver.pod.name`, default: `None`.
- `SPARK_CONNECT_KUBERNETES_EXECUTOR_REQUEST_CORES`: `spark.kubernetes.executor.cores`, default: `None`.
- `SPARK_CONNECT_KUBERNETES_EXECUTOR_LIMIT_CORES`: `spark.kubernetes.executor.limit.cores`, default: `None`.
- `SPARK_CONNECT_EXECUTOR_REQUEST_MEMORY`: `spark.executor.memory`, default: `512m`.
- `SPARK_CONNECT_EXECUTOR_LIMIT_MEMORY`: `spark.executor.memoryOverhead`, default: `None`.
- `SPARK_CONNECT_KUBERNETES_GPU_VENDOR`: `spark.executor.resource.gpu.vendor`, default: `nvidia.com`.
- `SPARK_CONNECT_KUBERNETES_GPU_DISCOVERY_SCRIPT`: `spark.executor.resource.gpu.discoveryScript`, default: `/opt/spark/examples/src/main/scripts/getGpusResources.sh`.
- `SPARK_CONNECT_KUBERNETES_GPU_AMOUNT`: `spark.executor.resource.gpu.amount`, default: `None`.
- `SPARK_CONNECT_RAPIDS_SQL_ENABLED`: `spark.rapids.sql.enabled`, default: `None`.


# TIPS

S3 secrets tokens(and others) need only be configured on the `Driver` or `Connect Server`, Configuration in `Connect client` take no effort.
